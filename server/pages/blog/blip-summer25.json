{"pageProps":{"post":{"date":"2024-08-01T09:00:00.000Z","slug":"blip-summer25","preview":"University Research Experience in collaboration with Georgia Tech researchers and the Wild Dolphin Project to build a whistle-controlled enrichment AUV. TinyML (LiteRT) classification on embedded hardware, an ESP32 and Google Pixel 6 based telemetry and control pipeline, and ROV control algorithms that adapt to dolphin acoustic patterns.","title":"Cetacean Enrichment AUV","tagline":"Research experience creating the Bioacoustic Listening and Interactive Platform (BLIP) AUV using ML to enable communication with humans and provide enjoyment for wild dolphins.","image":"/images/projects/dolphin/florida_scooter_summer25.jpg","content":"\r\n# Research Project Overview\r\n\r\nThis project has been my favorite part of Georgia Tech and has been an absolute blast.  This is a research project I do in my free time, however I was hired in Spring 25 under the Presidential Undergratuate Research Award at Georgia Tech.  This is a research project with the **Wild Dolphin Project** and **Google**, helping enable Dr. Denise Herzing with Google's Thad Starner *(the genius behind the Google Glass wearable)* to learn how Cetaceans (dolphins, belugas, whales, etc.) communicate.  Since this is being done for the Wild Dolphin Project, we primarily focus on dolphins in most cases.  This research has been done through wearable devices in previous years, but this process takes a long time between words and a lot of focus from the marine biologist researchers at the Wild Dolphin Project.\r\n\r\nTo help make this communication instantaneous, enriching for the animals, and easier for researchers to observe how cetaceans learn from an observable distance, we are creating an Autonomous Underwater Vehicle (AUV) as a sort of RC toy for the wild dolphins in the Bahamas.  It will listen to dolphins and react to certain actions the dolphins take, so they can learn how to control it and have fun making it do smooth, cool tricks and movements such as flips, turns, and dives.\r\n\r\n![Group Picture on the way to Pool Test](/images/projects/dolphin/dolphin_group_spring25.jpg)\r\n\r\n# How it works\r\n\r\nThis interactive platform works by using a hydrophone and ML algorithms to classify different whistle sequences and broadband click rates simultaneously.  Based on this data collected, we decide which movements to do and how fast to do it.  This then kicks the PID loops on - as we don't want the robot to move unless it receives a command from the dolphins to lessen confusion - and it smoothly performs the command provided while maintaining desired depth and pitch.  This results in a learnable sequence for the dolphins where they do an action and see an immediate result - whereas with humans, that would take many seconds for them to hear the request through their wearable CHAT system and then react a few seconds later.\r\n\r\n# What I contributed\r\n\r\nEmbedded TinyML pipeline: Integrated LiteRT-based classifiers on constrained hardware to recognize whistles and click patterns. Prototype combined an ESP32 for sensor telemetry and a Pixel device (field tests) for GPU inference before transitioning to an ROV architecture.\r\n\r\nCustom circuitry & firmware: Designed interfaces for hydrophones/IMU + motor controllers. Implemented real-time data pipelines to deliver classification results to the vehicle control loop.\r\n\r\nReal-time control algorithms: Implemented smooth trajectory controllers (PID-based smoothing and coordinated speed profiles) to produce natural-looking movement patterns triggered by acoustic cues. Safety and animal comfort were central constraints.\r\n\r\nResearch & deployment: Worked closely with field researchers to validate behavior, instrument the platform for underwater use, and iterate on interaction design.\r\n\r\n# Why it matters\r\n\r\nasdfasdf\r\n\r\n# Partners\r\n\r\n- **Thad Starner** - Georgia Tech Professor, Director of GT's Contextual Computing Group (CCG), Founder of GT's ACI Lab, Lead Researcher with GT's Ubicomp Group and Brainlab, and a Google Leader/Manager on the Google Glass\r\n- **Charles Ramey** - Georgia Tech PHD Student in Intelligent Systems and Lead Researcher in many Animal Lab research projects including this one\r\n- **Riley Mehrman** - Georgia Tech Masters Student in Mechanical Engineering tasked with designing the entire physical exterior including the exterior safety skin, waterproof seal, any mounting points, and much of the heavy electrical system ensuring safety and integrity\r\n- **Ojas Mediratta** - Georgia Tech Masters Student in Robotics tasked with implementing and checking the \r\n\r\n# References\r\n\r\nOfficial article about the research and model (DolphinGemma): https://blog.google/technology/ai/dolphingemma\r\n. \r\nblog.google\r\n\r\nDeepMind description of DolphinGemma: https://deepmind.google/models/gemma/dolphingemma\r\n. \r\nGoogle DeepMind"}},"__N_SSG":true}