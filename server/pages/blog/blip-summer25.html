<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width" data-next-head=""/><title data-next-head="">Blog - Cetacean Enrichment AUV</title><meta name="description" content="University Research Experience in collaboration with Georgia Tech researchers and the Wild Dolphin Project to build a whistle-controlled enrichment AUV. TinyML (LiteRT) classification on embedded hardware, an ESP32 and Google Pixel 6 based telemetry and control pipeline, and ROV control algorithms that adapt to dolphin acoustic patterns." data-next-head=""/><link rel="preload" href="/_next/static/css/f75252057ebd8fda.css" as="style"/><link rel="stylesheet" href="/_next/static/css/f75252057ebd8fda.css" data-n-g=""/><link rel="preload" href="/_next/static/css/9c941692721f3574.css" as="style"/><link rel="stylesheet" href="/_next/static/css/9c941692721f3574.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/_next/static/chunks/webpack-a0bed7097a96a01b.js" defer=""></script><script src="/_next/static/chunks/framework-d598bda8557cbfc3.js" defer=""></script><script src="/_next/static/chunks/main-bdb767e32da17446.js" defer=""></script><script src="/_next/static/chunks/pages/_app-66f3bfa4b141a0a2.js" defer=""></script><script src="/_next/static/chunks/281c3f4d-fdd6a2a234a28398.js" defer=""></script><script src="/_next/static/chunks/131-5beaed0820da83bd.js" defer=""></script><script src="/_next/static/chunks/428-37ad13333fedd283.js" defer=""></script><script src="/_next/static/chunks/230-acc20f64b3928d3e.js" defer=""></script><script src="/_next/static/chunks/124-f9ec780447fb90be.js" defer=""></script><script src="/_next/static/chunks/779-c26c62da993b87e9.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bslug%5D-b6e2c0672ae6c781.js" defer=""></script><script src="/_next/static/oPnSbqeQ2Aul3cEoy1qma/_buildManifest.js" defer=""></script><script src="/_next/static/oPnSbqeQ2Aul3cEoy1qma/_ssgManifest.js" defer=""></script></head><body><div id="__next"><script>!function(){try{var d=document.documentElement,n='data-theme',s='setAttribute';var e=localStorage.getItem('theme');if('system'===e||(!e&&true)){var t='(prefers-color-scheme: dark)',m=window.matchMedia(t);if(m.media!==t||m.matches){d.style.colorScheme = 'dark';d[s](n,'dark')}else{d.style.colorScheme = 'light';d[s](n,'light')}}else if(e){d[s](n,e|| '')}if(e==='light'||e==='dark')d.style.colorScheme=e}catch(e){}}()</script><div class="container mx-auto mt-10 undefined"><div class="block tablet:hidden mt-5"><div class="flex items-center justify-between p-2 laptop:p-0"><h1 class="font-medium p-2 laptop:p-0 link">Dawson Pent</h1><div class="flex items-center"><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link"><img class="h-6" src="/images/sun.svg"/></button><button id="headlessui-popover-button-:R4l6:" type="button" aria-expanded="false"><img class="h-5" src="/images/menu.svg"/></button></div></div></div><div class="mt-10 hidden flex-row items-center justify-between sticky false dark:text-white top-0 z-10 tablet:flex"><h1 class="font-medium cursor-pointer mob:p-2 laptop:p-0">Dawson Pent<!-- -->.</h1><div class="flex"><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">Home</button><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">Blog</button><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined first:ml-1 link">Resume</button><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">Contact</button></div></div><div class="mt-10 flex flex-col"><img class="w-full h-96 rounded-lg shadow-lg object-cover" src="/images/projects/dolphin/florida_scooter_summer25.jpg" alt="Cetacean Enrichment AUV"/><h1 class="mt-10 text-4xl mob:text-2xl laptop:text-6xl text-bold">Cetacean Enrichment AUV</h1><h2 class="mt-2 text-xl max-w-4xl text-darkgray opacity-50">Research experience creating the Bioacoustic Listening and Interactive Platform (BLIP) AUV using ML to enable communication with humans and provide enjoyment for wild dolphins.</h2></div><div class="markdown-class"><h1>Research Project Overview</h1>
<p>This project has been my favorite part of Georgia Tech and has been an absolute blast.  This is a research project I do in my free time, however I was hired in Spring 25 under the Presidential Undergratuate Research Award at Georgia Tech.  This is a research project with the <strong>Wild Dolphin Project</strong> and <strong>Google</strong>, helping enable Dr. Denise Herzing with Google&#x27;s Thad Starner <em>(the genius behind the Google Glass wearable)</em> to learn how Cetaceans (dolphins, belugas, whales, etc.) communicate.  Since this is being done for the Wild Dolphin Project, we primarily focus on dolphins in most cases.  This research has been done through wearable devices in previous years, but this process takes a long time between words and a lot of focus from the marine biologist researchers at the Wild Dolphin Project.</p>
<p>To help make this communication instantaneous, enriching for the animals, and easier for researchers to observe how cetaceans learn from an observable distance, we are creating an Autonomous Underwater Vehicle (AUV) as a sort of RC toy for the wild dolphins in the Bahamas.  It will listen to dolphins and react to certain actions the dolphins take, so they can learn how to control it and have fun making it do smooth, cool tricks and movements such as flips, turns, and dives.</p>
<p><div style="display:flex;justify-content:center"><img src="/images/projects/dolphin/dolphin_group_spring25.jpg" alt="Group Picture on the way to Pool Test" style="max-height:500px;width:auto;object-fit:contain"/></div></p>
<h1>How it works</h1>
<p>This interactive platform works by using a hydrophone and ML algorithms to classify different whistle sequences and broadband click rates simultaneously.  Based on this data collected, we decide which movements to do and how fast to do it.  This then kicks the PID loops on - as we don&#x27;t want the robot to move unless it receives a command from the dolphins to lessen confusion - and it smoothly performs the command provided while maintaining desired depth and pitch.  This results in a learnable sequence for the dolphins where they do an action and see an immediate result - whereas with humans, that would take many seconds for them to hear the request through their wearable CHAT system and then react a few seconds later.</p>
<h1>What I contributed</h1>
<p>Embedded TinyML pipeline: Integrated LiteRT-based classifiers on constrained hardware to recognize whistles and click patterns. Prototype combined an ESP32 for sensor telemetry and a Pixel device (field tests) for GPU inference before transitioning to an ROV architecture.</p>
<p>Custom circuitry &amp; firmware: Designed interfaces for hydrophones/IMU + motor controllers. Implemented real-time data pipelines to deliver classification results to the vehicle control loop.</p>
<p>Real-time control algorithms: Implemented smooth trajectory controllers (PID-based smoothing and coordinated speed profiles) to produce natural-looking movement patterns triggered by acoustic cues. Safety and animal comfort were central constraints.</p>
<p>Research &amp; deployment: Worked closely with field researchers to validate behavior, instrument the platform for underwater use, and iterate on interaction design.</p>
<h1>Why it matters</h1>
<p>asdfasdf</p>
<h1>Partners</h1>
<ul>
<li><strong>Thad Starner</strong> - Georgia Tech Professor, Director of GT&#x27;s Contextual Computing Group (CCG), Founder of GT&#x27;s ACI Lab, Lead Researcher with GT&#x27;s Ubicomp Group and Brainlab, and a Google Leader/Manager on the Google Glass</li>
<li><strong>Charles Ramey</strong> - Georgia Tech PHD Student in Intelligent Systems and Lead Researcher in many Animal Lab research projects including this one</li>
<li><strong>Riley Mehrman</strong> - Georgia Tech Masters Student in Mechanical Engineering tasked with designing the entire physical exterior including the exterior safety skin, waterproof seal, any mounting points, and much of the heavy electrical system ensuring safety and integrity</li>
<li><strong>Ojas Mediratta</strong> - Georgia Tech Masters Student in Robotics tasked with implementing and checking the</li>
</ul>
<h1>References</h1>
<p>Official article about the research and model (DolphinGemma): https://blog.google/technology/ai/dolphingemma
.
blog.google</p>
<p>DeepMind description of DolphinGemma: https://deepmind.google/models/gemma/dolphingemma
.
Google DeepMind</p></div><div class="mt-5 laptop:mt-40 p-2 laptop:p-0"><div class="mt-10"><div class="undefined flex flex-wrap mob:flex-nowrap link"><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">Github</button><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">LinkedIn</button><button type="button" class="text-sm tablet:text-base p-1 laptop:p-2 m-1 laptop:m-2 rounded-lg flex items-center transition-all ease-out duration-300 hover:bg-slate-100 hover:scale-105 active:scale-100  tablet:first:ml-0  undefined undefined link">Email</button></div></div></div><h1 class="text-sm text-bold mt-2 laptop:mt-10 p-2 laptop:p-0">Template from<!-- --> <a href="http://www.chetanverma.com">Chetan Verma</a> | <!-- -->Edited by Dawson</h1></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"date":"2024-08-01T09:00:00.000Z","slug":"blip-summer25","preview":"University Research Experience in collaboration with Georgia Tech researchers and the Wild Dolphin Project to build a whistle-controlled enrichment AUV. TinyML (LiteRT) classification on embedded hardware, an ESP32 and Google Pixel 6 based telemetry and control pipeline, and ROV control algorithms that adapt to dolphin acoustic patterns.","title":"Cetacean Enrichment AUV","tagline":"Research experience creating the Bioacoustic Listening and Interactive Platform (BLIP) AUV using ML to enable communication with humans and provide enjoyment for wild dolphins.","image":"/images/projects/dolphin/florida_scooter_summer25.jpg","content":"\r\n# Research Project Overview\r\n\r\nThis project has been my favorite part of Georgia Tech and has been an absolute blast.  This is a research project I do in my free time, however I was hired in Spring 25 under the Presidential Undergratuate Research Award at Georgia Tech.  This is a research project with the **Wild Dolphin Project** and **Google**, helping enable Dr. Denise Herzing with Google's Thad Starner *(the genius behind the Google Glass wearable)* to learn how Cetaceans (dolphins, belugas, whales, etc.) communicate.  Since this is being done for the Wild Dolphin Project, we primarily focus on dolphins in most cases.  This research has been done through wearable devices in previous years, but this process takes a long time between words and a lot of focus from the marine biologist researchers at the Wild Dolphin Project.\r\n\r\nTo help make this communication instantaneous, enriching for the animals, and easier for researchers to observe how cetaceans learn from an observable distance, we are creating an Autonomous Underwater Vehicle (AUV) as a sort of RC toy for the wild dolphins in the Bahamas.  It will listen to dolphins and react to certain actions the dolphins take, so they can learn how to control it and have fun making it do smooth, cool tricks and movements such as flips, turns, and dives.\r\n\r\n![Group Picture on the way to Pool Test](/images/projects/dolphin/dolphin_group_spring25.jpg)\r\n\r\n# How it works\r\n\r\nThis interactive platform works by using a hydrophone and ML algorithms to classify different whistle sequences and broadband click rates simultaneously.  Based on this data collected, we decide which movements to do and how fast to do it.  This then kicks the PID loops on - as we don't want the robot to move unless it receives a command from the dolphins to lessen confusion - and it smoothly performs the command provided while maintaining desired depth and pitch.  This results in a learnable sequence for the dolphins where they do an action and see an immediate result - whereas with humans, that would take many seconds for them to hear the request through their wearable CHAT system and then react a few seconds later.\r\n\r\n# What I contributed\r\n\r\nEmbedded TinyML pipeline: Integrated LiteRT-based classifiers on constrained hardware to recognize whistles and click patterns. Prototype combined an ESP32 for sensor telemetry and a Pixel device (field tests) for GPU inference before transitioning to an ROV architecture.\r\n\r\nCustom circuitry \u0026 firmware: Designed interfaces for hydrophones/IMU + motor controllers. Implemented real-time data pipelines to deliver classification results to the vehicle control loop.\r\n\r\nReal-time control algorithms: Implemented smooth trajectory controllers (PID-based smoothing and coordinated speed profiles) to produce natural-looking movement patterns triggered by acoustic cues. Safety and animal comfort were central constraints.\r\n\r\nResearch \u0026 deployment: Worked closely with field researchers to validate behavior, instrument the platform for underwater use, and iterate on interaction design.\r\n\r\n# Why it matters\r\n\r\nasdfasdf\r\n\r\n# Partners\r\n\r\n- **Thad Starner** - Georgia Tech Professor, Director of GT's Contextual Computing Group (CCG), Founder of GT's ACI Lab, Lead Researcher with GT's Ubicomp Group and Brainlab, and a Google Leader/Manager on the Google Glass\r\n- **Charles Ramey** - Georgia Tech PHD Student in Intelligent Systems and Lead Researcher in many Animal Lab research projects including this one\r\n- **Riley Mehrman** - Georgia Tech Masters Student in Mechanical Engineering tasked with designing the entire physical exterior including the exterior safety skin, waterproof seal, any mounting points, and much of the heavy electrical system ensuring safety and integrity\r\n- **Ojas Mediratta** - Georgia Tech Masters Student in Robotics tasked with implementing and checking the \r\n\r\n# References\r\n\r\nOfficial article about the research and model (DolphinGemma): https://blog.google/technology/ai/dolphingemma\r\n. \r\nblog.google\r\n\r\nDeepMind description of DolphinGemma: https://deepmind.google/models/gemma/dolphingemma\r\n. \r\nGoogle DeepMind"}},"__N_SSG":true},"page":"/blog/[slug]","query":{"slug":"blip-summer25"},"buildId":"oPnSbqeQ2Aul3cEoy1qma","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>